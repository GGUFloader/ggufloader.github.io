<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GGUF Memory Calculator - RAM Requirements for Q4_K_M, Q5_K_M, Q6_K (2025)</title>
    <meta name="description" content="Calculate GGUF model RAM requirements instantly. Memory usage calculator for Mistral 7B, Llama 3.2, Qwen 2.5 with Q4_K_M, Q5_K_M, Q6_K, Q8_0 quantization. Find the right model for your 8GB, 16GB, 32GB, or 64GB RAM system. Interactive calculator included.">
    <meta name="keywords" content="gguf memory calculator, mistral 7b gguf q4_k_m memory usage, gguf ram requirements, q4_k_m memory, q5_k_m size, q6_k ram, q8_0 memory, 7b model ram, 13b model ram, 70b model ram, gguf quantization memory, mistral 7b memory requirements, llama memory usage, llama 3.2 ram, qwen 2.5 memory, gguf size calculator, how much ram for gguf, gguf vram requirements, local ai memory">
    <meta name="author" content="Hussain Nazary">
    <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1">
    <link rel="canonical" href="https://ggufloader.github.io/gguf-memory-calculator.html">
    
    <!-- Open Graph -->
    <meta property="og:title" content="GGUF Memory Calculator - RAM Requirements for All Quantizations">
    <meta property="og:description" content="Calculate how much RAM you need for GGUF models. Covers Q4_K_M, Q5_K_M, Q6_K, Q8_0 for 7B, 13B, 70B models. Interactive calculator.">
    <meta property="og:image" content="https://ggufloader.github.io/preview.png">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:url" content="https://ggufloader.github.io/gguf-memory-calculator.html">
    <meta property="og:type" content="article">
    <meta property="og:site_name" content="GGUF Loader">
    <meta property="article:published_time" content="2025-12-01T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-12-25T00:00:00+00:00">
    <meta property="article:author" content="Hussain Nazary">
    <meta property="article:section" content="Tools">
    <meta property="article:tag" content="GGUF">
    <meta property="article:tag" content="Memory">
    <meta property="article:tag" content="Calculator">
    
    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="GGUF Memory Calculator - RAM Requirements">
    <meta name="twitter:description" content="Calculate RAM needs for GGUF models. Q4_K_M, Q5_K_M, Q6_K for 7B, 13B, 70B models.">
    <meta name="twitter:image" content="https://ggufloader.github.io/preview.png">
    
    <!-- JSON-LD Article Schema -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "TechArticle",
      "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://ggufloader.github.io/gguf-memory-calculator.html"
      },
      "headline": "GGUF Memory Calculator - RAM Requirements for All Quantizations",
      "description": "Calculate GGUF model RAM requirements. Memory usage for different model sizes and quantization types.",
      "image": "https://ggufloader.github.io/preview.png",
      "datePublished": "2025-12-01T00:00:00+00:00",
      "dateModified": "2025-12-25T00:00:00+00:00",
      "author": {
        "@type": "Person",
        "name": "Hussain Nazary"
      },
      "publisher": {
        "@type": "Organization",
        "name": "GGUF Loader",
        "logo": {
          "@type": "ImageObject",
          "url": "https://ggufloader.github.io/preview.png"
        }
      },
      "keywords": ["GGUF memory", "RAM calculator", "Q4_K_M", "Q5_K_M", "model size", "quantization"],
      "articleSection": "Tools"
    }
    </script>
    
    <!-- JSON-LD Table Schema -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Table",
      "about": "GGUF Model RAM Requirements",
      "description": "RAM requirements for different GGUF model sizes with Q4_K_M quantization"
    }
    </script>
    
    <!-- JSON-LD BreadcrumbList -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      "itemListElement": [
        {"@type": "ListItem", "position": 1, "name": "Home", "item": "https://ggufloader.github.io"},
        {"@type": "ListItem", "position": 2, "name": "Blog", "item": "https://ggufloader.github.io/blog.html"},
        {"@type": "ListItem", "position": 3, "name": "Memory Calculator", "item": "https://ggufloader.github.io/gguf-memory-calculator.html"}
      ]
    }
    </script>
    
    <!-- JSON-LD FAQ Schema -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [
        {
          "@type": "Question",
          "name": "How much RAM does Mistral 7B GGUF Q4_K_M need?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Mistral 7B with Q4_K_M quantization requires approximately 5-6GB RAM for the model itself, plus 1-2GB for context window. Total recommended: 8GB RAM minimum, 16GB for comfortable usage with other applications."
          }
        },
        {
          "@type": "Question",
          "name": "What is the memory footprint of Q4_K_M quantization?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Q4_K_M uses approximately 0.5-0.6 bytes per parameter. For a 7B model: ~4GB file size, ~5-6GB RAM usage. For a 13B model: ~7.5GB file size, ~9-10GB RAM usage."
          }
        },
        {
          "@type": "Question",
          "name": "Can I run a 7B GGUF model on 8GB RAM?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Yes, but barely. A 7B Q4_K_M model uses ~5-6GB RAM. With 8GB total, you'll have limited headroom for context and other apps. 16GB RAM is recommended for 7B models. For 8GB systems, use 1-3B models instead."
          }
        },
        {
          "@type": "Question",
          "name": "How much RAM per parameter for GGUF Q4_K_M?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Q4_K_M uses approximately 0.5-0.6 bytes per parameter in RAM. This means: 1B params ‚âà 0.6GB, 7B params ‚âà 4-5GB, 13B params ‚âà 7-8GB, 70B params ‚âà 35-40GB."
          }
        },
        {
          "@type": "Question",
          "name": "What's the difference between Q4_K_M and Q5_K_M memory usage?",
          "acceptedAnswer": {
            "@type": "Answer",
            "text": "Q5_K_M uses about 20% more memory than Q4_K_M but offers slightly better quality. For a 7B model: Q4_K_M ‚âà 5GB RAM, Q5_K_M ‚âà 6GB RAM, Q6_K ‚âà 7GB RAM."
          }
        }
      ]
    }
    </script>
    
    <link rel="stylesheet" href="styles.min.css">
    <link rel="stylesheet" href="mobile-fixes.css">
    <style>
        .article-container { max-width: 1000px; margin: 0 auto; padding: 20px; }
        .article-header { background: linear-gradient(135deg, #e74c3c 0%, #c0392b 100%); color: white; padding: 40px; border-radius: 15px; margin-bottom: 30px; }
        .article-header h1 { font-size: 2.2rem; margin-bottom: 15px; }
        .content-section { background: #fff; border-radius: 12px; padding: 30px; margin-bottom: 25px; box-shadow: 0 5px 20px rgba(0,0,0,0.08); }
        .content-section h2 { color: #2c3e50; border-bottom: 3px solid #e74c3c; padding-bottom: 10px; margin-bottom: 20px; }
        .memory-table { width: 100%; border-collapse: collapse; margin: 20px 0; }
        .memory-table th, .memory-table td { padding: 12px; text-align: center; border: 1px solid #e9ecef; }
        .memory-table th { background: #e74c3c; color: white; }
        .memory-table tr:nth-child(even) { background: #f8f9fa; }
        .memory-table tr:hover { background: #fff3cd; }
        .highlight { background: #27ae60; color: white; padding: 2px 8px; border-radius: 4px; font-weight: 600; }
        .warning { background: #f39c12; color: white; padding: 2px 8px; border-radius: 4px; }
        .danger { background: #e74c3c; color: white; padding: 2px 8px; border-radius: 4px; }
        .calculator-box { background: linear-gradient(135deg, #f8f9fa 0%, #e9ecef 100%); border-radius: 12px; padding: 25px; margin: 20px 0; border: 2px solid #e74c3c; }
        .calculator-box h3 { color: #e74c3c; margin-bottom: 15px; }
        .info-box { background: #f8f9fa; border-left: 4px solid #e74c3c; padding: 20px; margin: 20px 0; border-radius: 0 8px 8px 0; }
        .ram-recommendation { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 15px; margin: 20px 0; }
        .ram-card { background: #fff; border-radius: 10px; padding: 20px; text-align: center; box-shadow: 0 3px 10px rgba(0,0,0,0.1); border-top: 4px solid #e74c3c; }
        .ram-card h4 { color: #e74c3c; margin-bottom: 10px; }
        .back-link { display: inline-block; margin-bottom: 20px; color: #e74c3c; text-decoration: none; font-weight: 500; }
        code { background: #f1f3f4; padding: 2px 6px; border-radius: 4px; font-family: monospace; }
        
        /* Simulator Styles */
        .simulator-container {
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            border-radius: 16px;
            padding: 30px;
            color: white;
        }
        .sim-control {
            margin-bottom: 25px;
        }
        .sim-control label {
            display: block;
            font-size: 1.1rem;
            font-weight: 600;
            margin-bottom: 10px;
            color: #fff;
        }
        .sim-control input[type="range"] {
            width: 100%;
            height: 8px;
            border-radius: 4px;
            background: #374151;
            outline: none;
            -webkit-appearance: none;
        }
        .sim-control input[type="range"]::-webkit-slider-thumb {
            -webkit-appearance: none;
            width: 24px;
            height: 24px;
            border-radius: 50%;
            background: linear-gradient(135deg, #e74c3c, #c0392b);
            cursor: pointer;
            box-shadow: 0 2px 10px rgba(231, 76, 60, 0.5);
        }
        .sim-control select {
            width: 100%;
            padding: 12px 15px;
            font-size: 1rem;
            border: 2px solid #374151;
            border-radius: 8px;
            background: #1f2937;
            color: white;
            cursor: pointer;
        }
        .sim-control select:focus {
            border-color: #e74c3c;
            outline: none;
        }
        .sim-value {
            text-align: center;
            font-size: 1.5rem;
            font-weight: 700;
            color: #e74c3c;
            margin: 10px 0;
        }
        .ram-presets, .context-presets {
            display: flex;
            gap: 8px;
            justify-content: center;
            flex-wrap: wrap;
        }
        .ram-presets button, .context-presets button {
            padding: 8px 16px;
            border: 2px solid #374151;
            border-radius: 20px;
            background: transparent;
            color: #9ca3af;
            cursor: pointer;
            font-weight: 500;
            transition: all 0.2s;
        }
        .ram-presets button:hover, .context-presets button:hover {
            border-color: #e74c3c;
            color: #e74c3c;
        }
        .ram-presets button.active, .context-presets button.active {
            background: #e74c3c;
            border-color: #e74c3c;
            color: white;
        }
        
        /* Result Section */
        .sim-result {
            background: rgba(255,255,255,0.05);
            border-radius: 12px;
            padding: 25px;
            margin-top: 25px;
        }
        .ram-bar-container {
            margin-bottom: 20px;
        }
        .ram-bar-label {
            font-size: 0.9rem;
            color: #9ca3af;
            margin-bottom: 8px;
        }
        .ram-bar {
            height: 40px;
            background: #374151;
            border-radius: 8px;
            overflow: hidden;
            display: flex;
            position: relative;
        }
        .ram-bar-used {
            background: linear-gradient(90deg, #e74c3c, #c0392b);
            height: 100%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 600;
            font-size: 0.9rem;
            transition: width 0.5s ease;
            min-width: 60px;
        }
        .ram-bar-os {
            background: #f39c12;
            height: 100%;
            width: 0;
        }
        .ram-bar-free {
            background: #27ae60;
            height: 100%;
            flex: 1;
            transition: all 0.5s ease;
        }
        .ram-bar-legend {
            display: flex;
            gap: 20px;
            margin-top: 10px;
            font-size: 0.8rem;
            color: #9ca3af;
            flex-wrap: wrap;
        }
        .legend-used, .legend-os, .legend-free {
            display: inline-block;
            width: 12px;
            height: 12px;
            border-radius: 3px;
            margin-right: 5px;
            vertical-align: middle;
        }
        .legend-used { background: #e74c3c; }
        .legend-os { background: #f39c12; }
        .legend-free { background: #27ae60; }
        
        .result-details {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .result-item {
            background: rgba(255,255,255,0.05);
            padding: 12px;
            border-radius: 8px;
            text-align: center;
        }
        .result-label {
            display: block;
            font-size: 0.8rem;
            color: #9ca3af;
            margin-bottom: 5px;
        }
        .result-value {
            font-size: 1.2rem;
            font-weight: 700;
            color: #fff;
        }
        .result-total {
            color: #e74c3c;
        }
        
        .verdict {
            text-align: center;
            padding: 20px;
            border-radius: 12px;
            margin: 20px 0;
            transition: all 0.3s;
        }
        .verdict.success {
            background: linear-gradient(135deg, rgba(39, 174, 96, 0.2), rgba(39, 174, 96, 0.1));
            border: 2px solid #27ae60;
        }
        .verdict.warning {
            background: linear-gradient(135deg, rgba(243, 156, 18, 0.2), rgba(243, 156, 18, 0.1));
            border: 2px solid #f39c12;
        }
        .verdict.danger {
            background: linear-gradient(135deg, rgba(231, 76, 60, 0.2), rgba(231, 76, 60, 0.1));
            border: 2px solid #e74c3c;
        }
        .verdict-icon {
            font-size: 3rem;
            margin-bottom: 10px;
        }
        .verdict-text {
            font-size: 1.4rem;
            font-weight: 700;
            margin-bottom: 5px;
        }
        .verdict-detail {
            font-size: 0.9rem;
            color: #9ca3af;
        }
        
        .speed-indicator {
            margin-top: 20px;
        }
        .speed-label {
            font-size: 0.9rem;
            color: #9ca3af;
            margin-bottom: 8px;
        }
        .speed-bar {
            height: 12px;
            background: #374151;
            border-radius: 6px;
            overflow: hidden;
        }
        .speed-fill {
            height: 100%;
            background: linear-gradient(90deg, #e74c3c, #f39c12, #27ae60);
            border-radius: 6px;
            transition: width 0.5s ease;
        }
        .speed-text {
            text-align: center;
            margin-top: 8px;
            font-weight: 600;
            color: #27ae60;
        }
        
        .sim-recommendations {
            background: rgba(231, 76, 60, 0.1);
            border: 1px solid rgba(231, 76, 60, 0.3);
            border-radius: 12px;
            padding: 20px;
            margin-top: 20px;
        }
        .sim-recommendations h4 {
            color: #e74c3c;
            margin-bottom: 10px;
        }
        .sim-recommendations ul {
            margin: 0;
            padding-left: 20px;
            color: #d1d5db;
        }
        .sim-recommendations li {
            margin: 8px 0;
        }
        
        @media (max-width: 768px) {
            .simulator-container { padding: 20px; }
            .result-details { grid-template-columns: 1fr 1fr; }
            .ram-bar-legend { justify-content: center; }
        }
        @media (max-width: 768px) {
            .article-header { padding: 25px; }
            .article-header h1 { font-size: 1.5rem; }
            .content-section { padding: 20px; }
            .memory-table { font-size: 0.8rem; display: block; overflow-x: auto; }
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <a href="index.html" class="logo">GGUF Loader</a>
            <ul>
                <li><a href="index.html#features">Features</a></li>
                <li><a href="guides.html">Guides</a></li>
                <li><a href="faq.html">FAQ</a></li>
            </ul>
        </nav>
    </header>

    <main class="article-container">
        <a href="index.html" class="back-link">‚Üê Back to Home</a>
        
        <div class="article-header">
            <h1>GGUF Memory Calculator</h1>
            <p>RAM requirements for Q4_K_M, Q5_K_M, Q6_K quantization - Find the right model for your system</p>
        </div>

        <article>
            <!-- Interactive Simulator -->
            <section class="content-section" id="simulator">
                <h2>üéÆ Interactive RAM Simulator</h2>
                <p>Play with the sliders to see if a model will run on your system!</p>
                
                <div class="simulator-container">
                    <!-- Your System RAM -->
                    <div class="sim-control">
                        <label>üíª Your System RAM</label>
                        <input type="range" id="systemRam" min="4" max="128" value="16" step="4">
                        <div class="sim-value"><span id="systemRamValue">16</span> GB</div>
                        <div class="ram-presets">
                            <button onclick="setRam(8)">8GB</button>
                            <button onclick="setRam(16)" class="active">16GB</button>
                            <button onclick="setRam(32)">32GB</button>
                            <button onclick="setRam(64)">64GB</button>
                        </div>
                    </div>
                    
                    <!-- Model Selection -->
                    <div class="sim-control">
                        <label>ü§ñ Model Size</label>
                        <select id="modelSize" onchange="updateSimulator()">
                            <option value="1">TinyLlama 1.1B</option>
                            <option value="1.5">Llama 3.2 1B / Qwen 1.5B</option>
                            <option value="3">Llama 3.2 3B / Phi-3</option>
                            <option value="7" selected>Mistral 7B / Llama 3 8B</option>
                            <option value="13">Llama 2 13B</option>
                            <option value="34">CodeLlama 34B</option>
                            <option value="47">Mixtral 8x7B (MoE)</option>
                            <option value="70">Llama 2/3 70B</option>
                        </select>
                    </div>
                    
                    <!-- Quantization -->
                    <div class="sim-control">
                        <label>üìä Quantization</label>
                        <select id="quantization" onchange="updateSimulator()">
                            <option value="0.45">Q3_K_M (Smallest)</option>
                            <option value="0.55" selected>Q4_K_M (Recommended)</option>
                            <option value="0.65">Q5_K_M (Better Quality)</option>
                            <option value="0.75">Q6_K (High Quality)</option>
                            <option value="1.0">Q8_0 (Near Lossless)</option>
                            <option value="2.0">FP16 (Full Precision)</option>
                        </select>
                    </div>
                    
                    <!-- Context Length -->
                    <div class="sim-control">
                        <label>üìù Context Length</label>
                        <input type="range" id="contextLength" min="512" max="32768" value="4096" step="512">
                        <div class="sim-value"><span id="contextValue">4096</span> tokens</div>
                        <div class="context-presets">
                            <button onclick="setContext(2048)">2K</button>
                            <button onclick="setContext(4096)" class="active">4K</button>
                            <button onclick="setContext(8192)">8K</button>
                            <button onclick="setContext(16384)">16K</button>
                            <button onclick="setContext(32768)">32K</button>
                        </div>
                    </div>
                    
                    <!-- Visual Result -->
                    <div class="sim-result" id="simResult">
                        <div class="ram-bar-container">
                            <div class="ram-bar-label">RAM Usage Visualization</div>
                            <div class="ram-bar">
                                <div class="ram-bar-used" id="ramBarUsed">
                                    <span id="ramBarText">5.2 GB</span>
                                </div>
                                <div class="ram-bar-free" id="ramBarFree"></div>
                            </div>
                            <div class="ram-bar-legend">
                                <span><span class="legend-used"></span> Model + Context</span>
                                <span><span class="legend-os"></span> OS Reserved (~3GB)</span>
                                <span><span class="legend-free"></span> Free RAM</span>
                            </div>
                        </div>
                        
                        <div class="result-details">
                            <div class="result-item">
                                <span class="result-label">Model Size:</span>
                                <span class="result-value" id="resultModelSize">4.6 GB</span>
                            </div>
                            <div class="result-item">
                                <span class="result-label">Context Memory:</span>
                                <span class="result-value" id="resultContext">1.0 GB</span>
                            </div>
                            <div class="result-item">
                                <span class="result-label">Total Required:</span>
                                <span class="result-value result-total" id="resultTotal">5.6 GB</span>
                            </div>
                            <div class="result-item">
                                <span class="result-label">Available (after OS):</span>
                                <span class="result-value" id="resultAvailable">13 GB</span>
                            </div>
                        </div>
                        
                        <div class="verdict" id="verdict">
                            <div class="verdict-icon" id="verdictIcon">‚úÖ</div>
                            <div class="verdict-text" id="verdictText">Will run smoothly!</div>
                            <div class="verdict-detail" id="verdictDetail">You have 7.4 GB headroom for other apps</div>
                        </div>
                        
                        <div class="speed-indicator">
                            <div class="speed-label">Expected Speed:</div>
                            <div class="speed-bar">
                                <div class="speed-fill" id="speedFill"></div>
                            </div>
                            <div class="speed-text" id="speedText">~25-35 tokens/sec</div>
                        </div>
                    </div>
                    
                    <!-- Recommendations -->
                    <div class="sim-recommendations" id="simRecommendations">
                        <h4>üí° Recommendations</h4>
                        <ul id="recommendationsList">
                            <li>This configuration should work well on your system</li>
                        </ul>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2>Quick RAM Reference</h2>
                <p>How much RAM do you need for popular GGUF models? Here's a comprehensive reference:</p>
                
                <table class="memory-table">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Parameters</th>
                            <th>Q4_K_M RAM</th>
                            <th>Q5_K_M RAM</th>
                            <th>Q6_K RAM</th>
                            <th>Q8_0 RAM</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>TinyLlama</td>
                            <td>1.1B</td>
                            <td><span class="highlight">~1 GB</span></td>
                            <td>~1.2 GB</td>
                            <td>~1.4 GB</td>
                            <td>~1.5 GB</td>
                        </tr>
                        <tr>
                            <td>Llama 3.2 1B</td>
                            <td>1B</td>
                            <td><span class="highlight">~1.5 GB</span></td>
                            <td>~1.8 GB</td>
                            <td>~2 GB</td>
                            <td>~2.2 GB</td>
                        </tr>
                        <tr>
                            <td>Qwen 2.5 1.5B</td>
                            <td>1.5B</td>
                            <td><span class="highlight">~2 GB</span></td>
                            <td>~2.3 GB</td>
                            <td>~2.6 GB</td>
                            <td>~3 GB</td>
                        </tr>
                        <tr>
                            <td>Phi-2</td>
                            <td>2.7B</td>
                            <td>~3 GB</td>
                            <td>~3.5 GB</td>
                            <td>~4 GB</td>
                            <td>~5 GB</td>
                        </tr>
                        <tr>
                            <td>Llama 3.2 3B</td>
                            <td>3B</td>
                            <td>~3.5 GB</td>
                            <td>~4 GB</td>
                            <td>~4.5 GB</td>
                            <td>~5.5 GB</td>
                        </tr>
                        <tr>
                            <td><strong>Mistral 7B</strong></td>
                            <td>7B</td>
                            <td><span class="highlight">~5-6 GB</span></td>
                            <td>~6-7 GB</td>
                            <td>~7-8 GB</td>
                            <td>~9 GB</td>
                        </tr>
                        <tr>
                            <td>Llama 3 8B</td>
                            <td>8B</td>
                            <td>~6 GB</td>
                            <td>~7 GB</td>
                            <td>~8 GB</td>
                            <td>~10 GB</td>
                        </tr>
                        <tr>
                            <td>Llama 2 13B</td>
                            <td>13B</td>
                            <td>~9-10 GB</td>
                            <td>~11-12 GB</td>
                            <td>~13 GB</td>
                            <td>~15 GB</td>
                        </tr>
                        <tr>
                            <td>Mixtral 8x7B</td>
                            <td>47B (MoE)</td>
                            <td>~26 GB</td>
                            <td>~32 GB</td>
                            <td>~38 GB</td>
                            <td>~50 GB</td>
                        </tr>
                        <tr>
                            <td>Llama 2 70B</td>
                            <td>70B</td>
                            <td><span class="danger">~40 GB</span></td>
                            <td>~50 GB</td>
                            <td>~55 GB</td>
                            <td>~70 GB</td>
                        </tr>
                    </tbody>
                </table>
                
                <div class="info-box">
                    <strong>‚ö†Ô∏è Important:</strong> These are base model RAM requirements. Add 1-2GB for context window (more for longer contexts) and system overhead. Your OS and other apps also need RAM!
                </div>
            </section>

            <section class="content-section">
                <h2>RAM Recommendations by System</h2>
                
                <div class="ram-recommendation">
                    <div class="ram-card">
                        <h4>8GB RAM</h4>
                        <p><strong>Best models:</strong></p>
                        <p>TinyLlama 1.1B<br>Llama 3.2 1B<br>Qwen 2.5 1.5B</p>
                        <p><small>Use Q4_K_M quantization</small></p>
                    </div>
                    <div class="ram-card">
                        <h4>16GB RAM</h4>
                        <p><strong>Best models:</strong></p>
                        <p>All 1-3B models<br>Mistral 7B<br>Llama 3 8B</p>
                        <p><small>Q4_K_M or Q5_K_M</small></p>
                    </div>
                    <div class="ram-card">
                        <h4>32GB RAM</h4>
                        <p><strong>Best models:</strong></p>
                        <p>All 7B models<br>13B models<br>Mixtral 8x7B</p>
                        <p><small>Q5_K_M or Q6_K</small></p>
                    </div>
                    <div class="ram-card">
                        <h4>64GB+ RAM</h4>
                        <p><strong>Best models:</strong></p>
                        <p>70B models<br>Large MoE models</p>
                        <p><small>Any quantization</small></p>
                    </div>
                </div>
            </section>

            <section class="content-section">
                <h2>Mistral 7B Memory Deep Dive</h2>
                <p>Mistral 7B is one of the most popular models. Here's detailed memory info:</p>
                
                <div class="calculator-box">
                    <h3>Mistral 7B Q4_K_M Memory Breakdown</h3>
                    <table class="memory-table">
                        <tr>
                            <td><strong>Model weights</strong></td>
                            <td>~4.1 GB</td>
                        </tr>
                        <tr>
                            <td><strong>KV cache (2K context)</strong></td>
                            <td>~0.5 GB</td>
                        </tr>
                        <tr>
                            <td><strong>KV cache (8K context)</strong></td>
                            <td>~2 GB</td>
                        </tr>
                        <tr>
                            <td><strong>Compute buffers</strong></td>
                            <td>~0.5 GB</td>
                        </tr>
                        <tr>
                            <td><strong>Total (2K context)</strong></td>
                            <td><span class="highlight">~5-6 GB</span></td>
                        </tr>
                        <tr>
                            <td><strong>Total (8K context)</strong></td>
                            <td><span class="warning">~7-8 GB</span></td>
                        </tr>
                    </table>
                </div>
                
                <div class="info-box">
                    <strong>üí° Tip:</strong> For 16GB RAM systems running Mistral 7B, use 2K-4K context to leave room for your OS and other applications. Reduce context size if you experience slowdowns.
                </div>
            </section>

            <section class="content-section">
                <h2>Memory Per Parameter Formula</h2>
                <p>Quick formula to estimate RAM for any model:</p>
                
                <div class="calculator-box">
                    <h3>RAM Estimation Formula</h3>
                    <p><code>RAM (GB) = Parameters (B) √ó Bytes per Parameter √ó 1.2 (overhead)</code></p>
                    <br>
                    <table class="memory-table">
                        <thead>
                            <tr>
                                <th>Quantization</th>
                                <th>Bytes per Parameter</th>
                                <th>7B Model Size</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr>
                                <td>Q4_K_M</td>
                                <td>~0.55</td>
                                <td>~4.6 GB</td>
                            </tr>
                            <tr>
                                <td>Q5_K_M</td>
                                <td>~0.65</td>
                                <td>~5.5 GB</td>
                            </tr>
                            <tr>
                                <td>Q6_K</td>
                                <td>~0.75</td>
                                <td>~6.3 GB</td>
                            </tr>
                            <tr>
                                <td>Q8_0</td>
                                <td>~1.0</td>
                                <td>~8.4 GB</td>
                            </tr>
                            <tr>
                                <td>FP16</td>
                                <td>~2.0</td>
                                <td>~16.8 GB</td>
                            </tr>
                        </tbody>
                    </table>
                </div>
            </section>

            <section class="content-section">
                <h2>Context Window Impact</h2>
                <p>Longer context windows require more RAM. Here's how context affects memory:</p>
                
                <table class="memory-table">
                    <thead>
                        <tr>
                            <th>Context Length</th>
                            <th>7B Model Extra RAM</th>
                            <th>13B Model Extra RAM</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>2,048 tokens</td>
                            <td>+0.5 GB</td>
                            <td>+0.8 GB</td>
                        </tr>
                        <tr>
                            <td>4,096 tokens</td>
                            <td>+1 GB</td>
                            <td>+1.5 GB</td>
                        </tr>
                        <tr>
                            <td>8,192 tokens</td>
                            <td>+2 GB</td>
                            <td>+3 GB</td>
                        </tr>
                        <tr>
                            <td>32,768 tokens</td>
                            <td>+8 GB</td>
                            <td>+12 GB</td>
                        </tr>
                    </tbody>
                </table>
            </section>

            <section class="content-section">
                <h2>Related Resources</h2>
                <ul>
                    <li><a href="what-is-gguf.html">What is GGUF? Complete Format Guide</a></li>
                    <li><a href="2025-07-07-top-10-gguf-models-i5-16gb.html">Best GGUF Models for 16GB RAM</a></li>
                    <li><a href="how-to-run-gguf-models.html">How to Run GGUF Models Locally</a></li>
                </ul>
            </section>
        </article>
    </main>

    <footer>
        <p>&copy; 2025 GGUF Loader. All rights reserved.</p>
    </footer>
    
    <script src="mobile-menu.js" defer></script>
    
    <!-- Simulator JavaScript -->
    <script>
        // Initialize simulator
        document.addEventListener('DOMContentLoaded', function() {
            updateSimulator();
            
            // Add event listeners
            document.getElementById('systemRam').addEventListener('input', function() {
                document.getElementById('systemRamValue').textContent = this.value;
                updatePresetButtons('ram', this.value);
                updateSimulator();
            });
            
            document.getElementById('contextLength').addEventListener('input', function() {
                document.getElementById('contextValue').textContent = this.value;
                updatePresetButtons('context', this.value);
                updateSimulator();
            });
        });
        
        function setRam(value) {
            document.getElementById('systemRam').value = value;
            document.getElementById('systemRamValue').textContent = value;
            updatePresetButtons('ram', value);
            updateSimulator();
        }
        
        function setContext(value) {
            document.getElementById('contextLength').value = value;
            document.getElementById('contextValue').textContent = value;
            updatePresetButtons('context', value);
            updateSimulator();
        }
        
        function updatePresetButtons(type, value) {
            const container = type === 'ram' ? '.ram-presets' : '.context-presets';
            document.querySelectorAll(container + ' button').forEach(btn => {
                btn.classList.remove('active');
                if (parseInt(btn.textContent) === parseInt(value) || 
                    btn.textContent === value + 'GB' || 
                    btn.textContent === (value/1000) + 'K' ||
                    (btn.textContent === '2K' && value == 2048) ||
                    (btn.textContent === '4K' && value == 4096) ||
                    (btn.textContent === '8K' && value == 8192) ||
                    (btn.textContent === '16K' && value == 16384) ||
                    (btn.textContent === '32K' && value == 32768)) {
                    btn.classList.add('active');
                }
            });
        }
        
        function updateSimulator() {
            const systemRam = parseFloat(document.getElementById('systemRam').value);
            const modelParams = parseFloat(document.getElementById('modelSize').value);
            const bytesPerParam = parseFloat(document.getElementById('quantization').value);
            const contextLength = parseInt(document.getElementById('contextLength').value);
            
            // Calculate memory requirements
            const modelSize = modelParams * bytesPerParam * 1.1; // 10% overhead
            const contextMemory = (contextLength / 1000) * (modelParams / 7) * 0.25; // Rough estimate
            const totalRequired = modelSize + contextMemory;
            const osReserved = 3; // Reserve 3GB for OS
            const availableRam = systemRam - osReserved;
            const freeRam = availableRam - totalRequired;
            
            // Update display values
            document.getElementById('resultModelSize').textContent = modelSize.toFixed(1) + ' GB';
            document.getElementById('resultContext').textContent = contextMemory.toFixed(1) + ' GB';
            document.getElementById('resultTotal').textContent = totalRequired.toFixed(1) + ' GB';
            document.getElementById('resultAvailable').textContent = availableRam.toFixed(0) + ' GB';
            
            // Update RAM bar
            const usedPercent = Math.min((totalRequired / systemRam) * 100, 100);
            const osPercent = (osReserved / systemRam) * 100;
            document.getElementById('ramBarUsed').style.width = usedPercent + '%';
            document.getElementById('ramBarText').textContent = totalRequired.toFixed(1) + ' GB';
            
            // Update verdict
            const verdict = document.getElementById('verdict');
            const verdictIcon = document.getElementById('verdictIcon');
            const verdictText = document.getElementById('verdictText');
            const verdictDetail = document.getElementById('verdictDetail');
            
            verdict.classList.remove('success', 'warning', 'danger');
            
            if (freeRam >= 4) {
                verdict.classList.add('success');
                verdictIcon.textContent = '‚úÖ';
                verdictText.textContent = 'Will run smoothly!';
                verdictDetail.textContent = 'You have ' + freeRam.toFixed(1) + ' GB headroom for other apps';
            } else if (freeRam >= 1) {
                verdict.classList.add('warning');
                verdictIcon.textContent = '‚ö†Ô∏è';
                verdictText.textContent = 'Will run, but tight on RAM';
                verdictDetail.textContent = 'Only ' + freeRam.toFixed(1) + ' GB free - close other apps';
            } else if (freeRam >= -2) {
                verdict.classList.add('warning');
                verdictIcon.textContent = 'üò∞';
                verdictText.textContent = 'Might work with swapping';
                verdictDetail.textContent = 'Will be slow - consider smaller model or lower quant';
            } else {
                verdict.classList.add('danger');
                verdictIcon.textContent = '‚ùå';
                verdictText.textContent = 'Not enough RAM!';
                verdictDetail.textContent = 'Need ' + Math.abs(freeRam).toFixed(1) + ' GB more RAM';
            }
            
            // Update speed indicator
            const speedFill = document.getElementById('speedFill');
            const speedText = document.getElementById('speedText');
            let speedPercent, speedLabel;
            
            if (freeRam >= 8) {
                speedPercent = 90;
                speedLabel = '~30-50 tokens/sec (Fast)';
            } else if (freeRam >= 4) {
                speedPercent = 70;
                speedLabel = '~20-35 tokens/sec (Good)';
            } else if (freeRam >= 1) {
                speedPercent = 50;
                speedLabel = '~10-20 tokens/sec (Moderate)';
            } else if (freeRam >= -2) {
                speedPercent = 25;
                speedLabel = '~2-10 tokens/sec (Slow)';
            } else {
                speedPercent = 5;
                speedLabel = 'Too slow / Won\'t run';
            }
            
            speedFill.style.width = speedPercent + '%';
            speedText.textContent = speedLabel;
            
            // Update recommendations
            updateRecommendations(systemRam, modelParams, bytesPerParam, contextLength, freeRam);
        }
        
        function updateRecommendations(systemRam, modelParams, bytesPerParam, contextLength, freeRam) {
            const list = document.getElementById('recommendationsList');
            const recommendations = [];
            
            if (freeRam < 0) {
                recommendations.push('üî¥ Try a smaller model (e.g., ' + (modelParams > 7 ? '7B' : '1-3B') + ' parameters)');
                recommendations.push('üî¥ Use Q4_K_M or Q3_K_M quantization for lower memory');
                if (contextLength > 4096) {
                    recommendations.push('üî¥ Reduce context length to 2K-4K tokens');
                }
            } else if (freeRam < 2) {
                recommendations.push('üü° Close other applications before running');
                recommendations.push('üü° Consider Q4_K_M quantization if using higher');
                if (contextLength > 4096) {
                    recommendations.push('üü° Reduce context to 4K for better performance');
                }
            } else if (freeRam < 4) {
                recommendations.push('üü¢ Good configuration for your system');
                if (bytesPerParam < 0.65) {
                    recommendations.push('üí° You could try Q5_K_M for slightly better quality');
                }
            } else {
                recommendations.push('üü¢ Excellent! This will run very smoothly');
                if (modelParams < 7 && systemRam >= 16) {
                    recommendations.push('üí° You have room for a larger model (7B+)');
                }
                if (bytesPerParam < 0.75 && freeRam > 8) {
                    recommendations.push('üí° Try Q6_K or Q8_0 for better quality');
                }
                if (contextLength < 8192 && freeRam > 6) {
                    recommendations.push('üí° You can increase context length for longer conversations');
                }
            }
            
            // Model-specific tips
            if (modelParams >= 70) {
                recommendations.push('üìå 70B models work best with GPU offloading');
            }
            if (modelParams === 47) {
                recommendations.push('üìå Mixtral uses MoE - only ~12B active at once');
            }
            
            list.innerHTML = recommendations.map(r => '<li>' + r + '</li>').join('');
        }
    </script>
</body>
</html>
